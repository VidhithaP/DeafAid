### üß† Problem Statement

Hearing-impaired individuals often face difficulties in perceiving verbal communication and recognizing 
human presence, especially in unfamiliar or public environments. Traditional hearing aids and 
communication tools do not always address the broader need for situational awareness‚Äîsuch as 
identifying who is speaking, what is being said, or if someone is approaching. This gap creates a 
communication barrier and increases dependency on others. Therefore, there is a need for a portable, 
intelligent module that can detect nearby people, identify visible objects, and transcribe spoken words in 
real-time to assist hearing-impaired individuals in maintaining independence and safety.

### üéØ Aim of the project

The aim of the project is to create a smart, real-time assistive device that bridges the communication gap for 
hearing-impaired individuals by integrating advanced AI technologies for visual and auditory interpretation. This 
system intends to improve accessibility, independence, and safety by enabling users to better understand their 
environment and interact with others more effectively. It is especially focused on being functional in everyday 
situations, whether at home, in public spaces, or while traveling.

### üí° Proposed Solution

The proposed system aims to address the limitations of existing solutions by integrating multiple
machine learning models into a unified, real-time assistive module. This project utilizes TensorFlow and 
TensorFlow Lite to build a lightweight, embedded-friendly system that can detect human presence, 
recognize the object a person is holding, and simultaneously transcribe their speech into text. Unlike 
traditional systems, this approach enables contextual awareness for hearing-impaired users in dynamic 
environments, such as public places or while traveling. This comprehensive approach significantly 
enhances independence and safety for deaf users, representing a leap forward in assistive technology. 

### üõ†Ô∏è Tools & Technologies Used

- HTML/CSS/JavaScript for building the responsive UI. 
- TensorFlow.js for loading and running the COCO-SSD object detection model in the browser. 
- Web Speech API for speech-to-text transcription. 
- Modern Browsers (Chrome/Firefox) for accessing the camera and microphone via 
navigator.mediaDevices. 





